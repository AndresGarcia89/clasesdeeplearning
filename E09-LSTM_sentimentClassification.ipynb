{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grupo conformado por:\n",
    "Edgar Andrés García Hernández - 200512532\n",
    "John Pablo Calvo López - 201726690"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 09\n",
    "\n",
    "## Sequence Classification using LSTM\n",
    "\n",
    "Sequence classification is a predictive modeling problem where you have some sequence of inputs over space or time and the task is to predict a category for the sequence.\n",
    "\n",
    "What makes this problem difficult is that the sequences can vary in length, be comprised of a very large vocabulary of input symbols and may require the model to learn the long-term context or dependencies between symbols in the input sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[The Large Movie Review Dataset](http://ai.stanford.edu/~amaas/data/sentiment/) (often referred to as the IMDB dataset) contains 25,000 highly-polar movie reviews (good or bad) for training and the same amount again for testing. The problem is to determine whether a given movie review has a positive or negative sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.client.session.Session at 0x289aad8a978>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.datasets import imdb\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "from keras.callbacks import History\n",
    "np.random.seed(7)\n",
    "\n",
    "import tensorflow as tf\n",
    "config = tf.ConfigProto()\n",
    "config.intra_op_parallelism_threads = 4\n",
    "config.inter_op_parallelism_threads = 4\n",
    "tf.Session(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from livelossplot import PlotLossesKeras\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataset but only keep the top n words, zero the rest\n",
    "top_words = 5000\n",
    "index_from = 3\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=top_words, index_from=index_from)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>col_0</th>\n",
       "      <th>Count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>row_0</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "col_0  Count\n",
       "row_0       \n",
       "0      12500\n",
       "1      12500"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.crosstab(y_train,\"Count\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 4999)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.min([np.min(x) for x in X_train]), np.max([np.max(x) for x in X_train])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The words have been replaced by integers that indicate the ordered frequency of each word in the dataset. The sentences in each review are therefore comprised of a sequence of integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000,)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 2, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 2, 336, 385, 39, 4, 172, 4536, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 4613, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 2, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 2, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 2, 8, 4, 107, 117, 2, 15, 256, 4, 2, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 2, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 2, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 2, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 4472, 113, 103, 32, 15, 16, 2, 19, 178, 32]\n"
     ]
    }
   ],
   "source": [
    "print(X_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need to truncate and pad the input sequences so that they are all the same length for modeling. The model will learn the zero values carry no information so indeed the sequences are not the same length in terms of content, but same length vectors is required to perform the computation in Keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# truncate and pad input sequences\n",
    "max_review_length = 500\n",
    "X_train_pad = sequence.pad_sequences(X_train, maxlen=max_review_length)\n",
    "X_test_pad = sequence.pad_sequences(X_test, maxlen=max_review_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 500)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_pad.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Embedding\n",
    "\n",
    "We will map each movie review into a real vector domain, a popular technique when working with text called word embedding. This is a technique where words are encoded as real-valued vectors in a high dimensional space, where the similarity between words in terms of meaning translates to closeness in the vector space.\n",
    "\n",
    "Keras provides a convenient way to convert positive integer representations of words into a word embedding by an Embedding layer.\n",
    "\n",
    "We will map each word onto a 32 length real valued vector. We will also limit the total number of words that we are interested in modeling to the 5000 most frequent words, and zero out the rest. Finally, the sequence length (number of words) in each review varies, so we will constrain each review to be 500 words, truncating long reviews and pad the shorter reviews with zero values.\n",
    "\n",
    "Now that we have defined our problem and how the data will be prepared and modeled, we are ready to develop an LSTM model to classify the sentiment of movie reviews."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 09.1\n",
    "\n",
    "Train a Deep Neural Network with the following architecture:\n",
    "\n",
    "- Input = pad_sequences (input_length=max_review_length)\n",
    "- Embedding(top_words, 32, input_length)\n",
    "- LSTM(100)\n",
    "- Dense(1, sigmoid)\n",
    "\n",
    "Optimized using adam using as loss binary_crossentropy\n",
    "\n",
    "Hints: \n",
    "- test with two iterations then try more. \n",
    "- learning can be ajusted\n",
    "\n",
    "Evaluate the performance using the testing set (aprox 87% with 10 epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.layers.core import Dense, Dropout\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "from livelossplot import PlotLossesKeras\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 500, 32)           160000    \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 100)               53200     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 213,301\n",
      "Trainable params: 213,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# A continuación se define el modelo\n",
    "\n",
    "mlstm = Sequential()\n",
    "\n",
    "mlstm.add(Embedding(top_words,32, input_length=max_review_length))\n",
    "mlstm.add(LSTM(100))\n",
    "mlstm.add(Dense(1, activation='sigmoid'))\n",
    "mlstm.compile(loss='binary_crossentropy', optimizer=Adam(lr=0.0001), metrics=['accuracy'])\n",
    "\n",
    "mlstm.summary()        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/15\n",
      "25000/25000 [==============================] - 586s 23ms/step - loss: 0.5744 - acc: 0.7086 - val_loss: 0.4846 - val_acc: 0.7658\n",
      "Epoch 2/15\n",
      "25000/25000 [==============================] - 1177s 47ms/step - loss: 0.3329 - acc: 0.8684 - val_loss: 0.3272 - val_acc: 0.8624\n",
      "Epoch 3/15\n",
      "25000/25000 [==============================] - 1276s 51ms/step - loss: 0.2685 - acc: 0.8968 - val_loss: 0.3215 - val_acc: 0.8704\n",
      "Epoch 4/15\n",
      "25000/25000 [==============================] - 1256s 50ms/step - loss: 0.2370 - acc: 0.9110 - val_loss: 0.3064 - val_acc: 0.8782\n",
      "Epoch 5/15\n",
      "25000/25000 [==============================] - 1272s 51ms/step - loss: 0.2178 - acc: 0.9187 - val_loss: 0.2819 - val_acc: 0.8835\n",
      "Epoch 6/15\n",
      "25000/25000 [==============================] - 838s 34ms/step - loss: 0.2011 - acc: 0.9281 - val_loss: 0.2901 - val_acc: 0.8806\n",
      "Epoch 7/15\n",
      "25000/25000 [==============================] - 496s 20ms/step - loss: 0.1929 - acc: 0.9299 - val_loss: 0.2873 - val_acc: 0.8797\n",
      "Epoch 8/15\n",
      "25000/25000 [==============================] - 449s 18ms/step - loss: 0.1795 - acc: 0.9367 - val_loss: 0.3550 - val_acc: 0.8706\n",
      "Epoch 9/15\n",
      "25000/25000 [==============================] - 476s 19ms/step - loss: 0.1768 - acc: 0.9368 - val_loss: 0.3073 - val_acc: 0.8748\n",
      "Epoch 10/15\n",
      "25000/25000 [==============================] - 493s 20ms/step - loss: 0.1678 - acc: 0.9406 - val_loss: 0.3037 - val_acc: 0.8761\n",
      "Epoch 11/15\n",
      "25000/25000 [==============================] - 492s 20ms/step - loss: 0.1602 - acc: 0.9443 - val_loss: 0.3091 - val_acc: 0.8751\n",
      "Epoch 12/15\n",
      "25000/25000 [==============================] - 492s 20ms/step - loss: 0.1607 - acc: 0.9444 - val_loss: 0.3502 - val_acc: 0.8711\n",
      "Epoch 13/15\n",
      "25000/25000 [==============================] - 505s 20ms/step - loss: 0.1490 - acc: 0.9490 - val_loss: 0.3455 - val_acc: 0.8702\n",
      "Epoch 14/15\n",
      "25000/25000 [==============================] - 501s 20ms/step - loss: 0.1429 - acc: 0.9506 - val_loss: 0.3682 - val_acc: 0.8668\n",
      "Epoch 15/15\n",
      "25000/25000 [==============================] - 501s 20ms/step - loss: 0.1401 - acc: 0.9526 - val_loss: 0.3493 - val_acc: 0.8700\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x289c9fac2e8>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Acto seguido, se entrena. Se utilizan quince épocas: para este caso particular, más épocas creaban overfitting y menos épocas no predecían lo suficientemente bien.\n",
    "\n",
    "mlstm.fit(X_train_pad, y_train, epochs=15,verbose=1,validation_data=(X_test_pad,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 09.2\n",
    "\n",
    "Predict the sentiment of the following reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = [\"I was fortunate enough to see this movie on pre-release last night and, though I wasn't expecting to, actually really enjoyed the movie for the most part. The rescues and sea effects were amazing to watch and definitely provided edge of the seat tense moments, probably all the more so knowing that there are guys who do this for a living. The weaker parts of the movie revolve largely around using stereotypical set scenes. I'm not going to spoil the movie but this really follows along the lines of An Officer and a Gentleman and those moments give it a little bit of a cheesy aftertaste.<br /><br />Like I said over all this movie is pretty good and worth checking out as long as you can get past the clichés.\",\n",
    "           '\"The Dresser\" is perhaps the most refined of backstage films. The film is brimming with wit and spirit, for the most part provided by the \"energetic\" character of Norman (Tom Courtenay). Although his character is clearly gay, and certainly has an attraction for the lead performer (Albert Finney) that he assists, the film never dwells on it or makes it more than it is.<br /><br />The gritty style of Peter Yates that worked so well in \"Bullitt\" is again on display, and gives the film a sense of realism and coherence. This is much appreciated in a story that could so easily have become tedious. In the end, \"The Dresser\" will bore many people silly, but it will truly be a delight to those who love British cinema.<br /><br />7.7 out of 10',\n",
    "           \"So real and surreal, all in one. I remember feeling like Tessa. Heck, I remember being Tessa. This was a beautiful vignette of a relationship ending. I especially liked the protesters tangent. It is nice to see symbolism in a movie without being smacked over the head with it. If you get the chance to see this, take it. It is well worth the 30 minutes.\",\n",
    "           \"This is a pale imitation of 'Officer and a Gentleman.' There is NO chemistry between Kutcher and the unknown woman who plays his love interest. The dialog is wooden, the situations hackneyed. It's too long and the climax is anti-climactic(!). I love the USCG, its men and women are fearless and tough. The action scenes are awesome, but this movie doesn't do much for recruiting, I fear. The script is formulaic, but confusing. Kutcher's character is trying to redeem himself for an accident that wasn't his fault? Costner's is raging against the dying of the light, but why? His 'conflict' with his wife is about as deep as a mud puddle. I saw this sneak preview for free and certainly felt I got my money's worth.\",\n",
    "           \"I was at Wrestlemania VI in Toronto as a 10 year old, and the event I saw then was pretty different from what I saw on the Wrestlemania Collection DVD I just watched. I don't understand how the wwE doesn't have the rights to some of the old music, since most of those songs were created by the WWF they shouldn't have to worry about the licensing and royalty fees that prevent shows like SNL from releasing season sets. Its pretty stupid to whine about, but for me hearing Demolition come out to their theme music at a Wrestlemania in person was a memory that I never forgot, and it didn't exist on this DVD. What is the point of them even owning the rights to this huge library of video if they have to edit it so drastically to use it?\",\n",
    "           \"Wow! What a movie if you want to blow your budget on the title and have it look real bad ask the guys that made this movie on how to do that. They could have spent the money on a good rewrite or something else. Or they could have spent it on beer when they made this movie at least it would have come out better.\"\n",
    "          ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reviews must be preprocessed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se importa un dicccionario de palabras\n",
    "\n",
    "from keras.datasets.imdb import get_word_index\n",
    "\n",
    "vocab = get_word_index()\n",
    "\n",
    "vocab = {k:(v+index_from) for k,v in vocab.items()}\n",
    "vocab[\"<PAD>\"] = 0\n",
    "vocab[\"<START>\"] = 1\n",
    "vocab[\"<UNK>\"] = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fawn': 34704,\n",
       " 'tsukino': 52009,\n",
       " 'nunnery': 52010,\n",
       " 'sonja': 16819,\n",
       " 'vani': 63954,\n",
       " 'woods': 1411,\n",
       " 'spiders': 16118,\n",
       " 'hanging': 2348,\n",
       " 'woody': 2292,\n",
       " 'trawling': 52011}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Se asigna un número a esas palabras del diccionario\n",
    "\n",
    "{k:vocab[k] for k in list(vocab.keys())[:10]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A continuación se convierten los análisis en números\n",
    "# Primero, es necesario asegurarse que dichos análisis sean convertidos a número palabra por palabra y no letra por letra\n",
    "# Para ello, se agrega esta línea\n",
    "\n",
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "X2 = [text_to_word_sequence(x) for x in reviews]\n",
    "\n",
    "# Posteriormente, se finaliza la conversión a números.\n",
    "# También se agrega un 1 al inicio de cada análisis, para indicar el \"<START>\" que tienen todas las observaciones en la base de entrenamiento\n",
    "\n",
    "X2 = [[vocab[x1] for x1 in x if x1 in vocab.keys()] for x in X2]\n",
    "for i in range(0,6,1):\n",
    "    X2[i]=np.asarray(X2[i])\n",
    "    X2[i]=np.insert(X2[i],0,1)\n",
    "len(X2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<START> i was fortunate enough to see this movie on pre release last night and though i wasn't expecting to actually really enjoyed the movie for the most part the rescues and sea effects were amazing to watch and definitely provided edge of the seat tense moments probably all the more so knowing that there are guys who do this for a living the weaker parts of the movie revolve largely around using stereotypical set scenes i'm not going to spoil the movie but this really follows along the lines of an officer and a gentleman and those moments give it a little bit of a cheesy aftertaste br br like i said over all this movie is pretty good and worth checking out as long as you can get past the clichés\n"
     ]
    }
   ],
   "source": [
    "# Para verificar que la codificación haya sido correcta, se revierte en un ejemplo. Se puede observar que los resultados fueron satisfactorios\n",
    "\n",
    "id_to_word={value:key for key,value in vocab.items()}\n",
    "print(' '.join(id_to_word[id] for id in X2[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6, 500)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tras lo anterior, se estandariza el formato de los análisis a secuencias, siguiendo el ejemplo de X_train_pad\n",
    "\n",
    "X2_pad = sequence.pad_sequences(X2, maxlen=500)\n",
    "X2_pad.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Habían palabras que se salían del diccionario de 5 mil palabras utilizado para la red neuronal\n",
    "# Dichas palabras fueron cambiadas por \"<UNK>\"\n",
    "\n",
    "X2_pad[X2_pad>5000]=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <START> i was <UNK> enough to see this movie on pre release last night and though i wasn't expecting to actually really enjoyed the movie for the most part the <UNK> and sea effects were amazing to watch and definitely provided edge of the seat tense moments probably all the more so knowing that there are guys who do this for a living the <UNK> parts of the movie <UNK> largely around using stereotypical set scenes i'm not going to spoil the movie but this really follows along the lines of an officer and a <UNK> and those moments give it a little bit of a cheesy <UNK> br br like i said over all this movie is pretty good and worth checking out as long as you can get past the clichés\n"
     ]
    }
   ],
   "source": [
    "# A continuación se verifica el resultado final. Nuevamente, sigue el mismo formato de X_train_pad\n",
    "\n",
    "id_to_word={value:key for key,value in vocab.items()}\n",
    "print(' '.join(id_to_word[id] for id in X2_pad[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.9939213 ],\n",
       "       [0.9999025 ],\n",
       "       [0.9977113 ],\n",
       "       [0.21613555],\n",
       "       [0.1500411 ],\n",
       "       [0.04357311]], dtype=float32)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Finalmente, se crean las predicciones\n",
    "\n",
    "y2_pred = mlstm.predict(X2_pad)\n",
    "y2_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se observa cómo el modelo clasificó a los tres primeros análisis como positivos y a los tres últimos como negativos. Lo anterior coincide con lo que se esperaría tras leer dichos análisis, lo cual habla muy bien de la precisión del modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
